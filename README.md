# Introduction

## About the tool
This Python tool allows to scrape tens of thousands of Medium articles in a few minutes based on some search term.

This tools has been actually used in professional projects, although here I do not share the finished version (for confidentiality purposes) but one of the first versions. In any case, the principles are the same.

The origin of the project started with a machine learning company that was training some Natural Language Processing models on a great variety of texts.
They needed to find blog articles about certain topics, and for that they hired freelancers. I was one of those freelancers.
My job was to provide them with a way of downloading tens of thousands of blog articles (indexed and filtered) in text format about some topics in the least amount of time possible (less than 10 minutes).
I built a solution using web scraping, multithreading and queues that would download those articles from the Medium.com archive.

This first version is not the one I ended up using and it is not the one I would recommend using in a professional setting. I would actually recommend a mix of concurrency and multithreading instead of 100% multithreading, but in any case this can be very useful if you are looking for a fully functional project that uses these technologies.

![diagram](Diagram.svg)

## Approach

The web scraping task was performed using Selenium and BeautifulSoup. Some details might need to be updated as the Medium interface can change over time.

The multithreading and queues implementations used are the native ones in Python 3.

## Conclusions
One of the lessons learned during the development was that the Medium infinite scroll is not infinite. It is limited to 1000 articles, and that made the use of the Medium default search bar useless.
We had to develop our custom way of going through the search results in the Medium archive results.

Also, a big proportion of articles are unreadable by humans or at least extremely unclear and incoherent and short (probably created by bots). We developed a way of filtering them before downloading them.

# Setup

## Installation of packages
The application needs to be executed in an environment with Python 3.8 installed.

If this is the case, proceed with the installation of necessary packages. Open the CLI (Command Line Interface) and execute:
```bash
pip install -r requirements.txt
```

## Execution
Execute the command 
```bash
jupyter notebook
```
to open the Jupyter Notebook and see how to use the program.

## Test
Test the application with Pytest using
```bash
python -m pytest
```

## Use your own AWS Lambda function
In order to setup a Lambda function in AWS, you must complete several steps.

First, create an account.

Then, create a user with administrator permissions so you don't use your root user (which can handle very sensitive information and operations).
Follow the steps in [here](https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_create-admin-group.html).

AWS will generate a csv file with the credentials for that user (i.e. private and public keys).
Associate that those credentials to a profile locally. Follow the steps in [here](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html)
and [here](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html).

Once done, follow the instructions from this [repository](https://github.com/awsdocs/aws-lambda-developer-guide/tree/main/sample-apps/blank-python)
to develop a Lambda function locally and then push it to AWS.

### Create bucket for artifacts
According to the instructions in that repository, the first task is to create a bucket to store the artifacts generated by the function.
The command is
```shell script
aws s3 mb s3://<bucket_name> --profile <profile_name>
```

You can just execute the script `aws_deployment/1_create_aws_bucket.sh` from the `aws_deployment` folder.

### Package Python distribution
The second step is to create a Python distribution with all the required packages. This is like creating a virtual environment,
but in this case you control where it is located with the `--target` flag in the `pip install -r requirements.txt` command.

You can just execute the script `aws_deployment/2_build_layer.sh`.

### Deploy in AWS
The third step is to deploy.

We divide this task into two steps:
- Uploading the necessary files to the previously created bucket
- Deploying the Lambda function

In order to do this comfortably, AWS has an API called [Cloudformation](https://docs.aws.amazon.com/cloudformation/index.html)
that allows you to define the infrastructure you want with a simple YAML template file. It is what is known as infrastructure as code.
Consult the AWS CLI CloudFormation command reference (`aws cloudformation ...`) in [here](https://docs.aws.amazon.com/cli/latest/reference/cloudformation/index.html).

The template is situated in `aws_deployment/cloudformation_template.yml`.
There are some parameters that refer to local locations, like the `aws_deployment/package` location.
Those are the ones that must be uploaded to the bucket.
You can do it with the command `aws cloudformation package`.
It will upload the files and then it will create another cloudformation template file with the locations changed from local to the bucket locations.

The deployment is done with the `aws cloudformation deploy` command.

You can execute both tasks with `aws_deployment/3_deploy.sh`.
